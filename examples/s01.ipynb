{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513f9dc-ea81-4a62-b70c-e3428be6e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Any, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bad63b-056c-4284-bb90-f4abbbf3bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteDataError(IOError):\n",
    "    pass\n",
    "\n",
    "def _init_session(session):\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    else:\n",
    "        if not isinstance(session, requests.Session):\n",
    "            raise TypeError(\"session must be a request.Session\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adae012-1983-4115-9730-85f678a531fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseReader:\n",
    "    \"\"\"\n",
    "    Base class for data readers with retry and session management.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request.\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries.\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds.\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "    ) -> None:\n",
    "        if not isinstance(retry_count, int) or retry_count < 0:\n",
    "            raise ValueError(\"'retry_count' must be integer larger than 0\")\n",
    "        if not isinstance(pause, (int, float)) or pause < 0:\n",
    "            raise ValueError(\"'pause' must be a positive number\")\n",
    "        if not isinstance(timeout, int) or timeout <= 0:\n",
    "            raise ValueError(\"'timeout' must be a positive integer\")\n",
    "        \n",
    "        self.retry_count = retry_count\n",
    "        self.pause = pause\n",
    "        self.timeout = timeout\n",
    "        self.pause_multiplier = 1\n",
    "        self.session = _init_session(session)\n",
    "        self.headers: Optional[Dict[str, str]] = None\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close network session.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "    @property\n",
    "    def url(self) -> str:\n",
    "        \"\"\"API URL - must be overridden in subclass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parameters to use in API calls.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def read(self) -> pd.DataFrame:\n",
    "        \"\"\"Read data from connector.\"\"\"\n",
    "        try:\n",
    "            return self._read_one_data(self.url, self.params)\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def read_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"Read data from connector and return as raw JSON.\"\"\"\n",
    "        try:\n",
    "            response = self._get_response(self.url, params=self.params)\n",
    "            return response.json()\n",
    "        finally:\n",
    "            self.close()\n",
    "    \n",
    "    def _read_one_data(self, url: str, params: Optional[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"Read one data from specified URL.\"\"\"\n",
    "        out = self._get_response(url, params=params).json()\n",
    "        return self._read_lines(out)\n",
    "\n",
    "    def _get_response(\n",
    "        self, \n",
    "        url: str, \n",
    "        params: Optional[Dict[str, Any]] = None, \n",
    "        headers: Optional[Dict[str, str]] = None\n",
    "    ) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Send raw HTTP request to get requests.Response from the specified url.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            Target URL\n",
    "        params : Optional[Dict[str, Any]]\n",
    "            Parameters passed to the URL\n",
    "        headers : Optional[Dict[str, str]]\n",
    "            Headers for the request\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        requests.Response\n",
    "            Response object from the HTTP request\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        RemoteDataError\n",
    "            If unable to retrieve data after all retry attempts\n",
    "        \"\"\"\n",
    "        headers = headers or self.headers\n",
    "        pause = self.pause\n",
    "        last_response_text = \"\"\n",
    "        \n",
    "        for _ in range(self.retry_count + 1):\n",
    "            response = self.session.get(\n",
    "                url, params=params, headers=headers, timeout=self.timeout\n",
    "            )\n",
    "            if response.status_code == requests.codes.ok:\n",
    "                return response\n",
    "\n",
    "            if response.encoding:\n",
    "                last_response_text = response.text.encode(response.encoding)\n",
    "            time.sleep(pause)\n",
    "\n",
    "            # Increase time between subsequent requests, per subclass.\n",
    "            pause *= self.pause_multiplier\n",
    "\n",
    "            # If our output error function returns True, exit the loop.\n",
    "            if self._output_error(response):\n",
    "                break\n",
    "\n",
    "        # If we reach here, we have exhausted all retries.\n",
    "        if params is not None and len(params) > 0:\n",
    "            url = url + \"?\" + urllib.parse.urlencode(query=params)\n",
    "        msg = f\"Unable to read URL: {url}\"\n",
    "        if last_response_text:\n",
    "            msg += f\"\\nResponse Text:\\n{last_response_text}\"\n",
    "\n",
    "        raise RemoteDataError(msg)\n",
    "\n",
    "    def _read_lines(self, out: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process JSON response into DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        out : Dict[str, Any]\n",
    "            JSON response data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        rs = pd.json_normalize(out, sep=\"_\")\n",
    "        # Remove blank space character in header names\n",
    "        rs = rs.assign(**{\n",
    "            col.strip(): rs[col] for col in rs.columns\n",
    "        }).drop(columns=rs.columns.tolist())\n",
    "\n",
    "        # Get rid of unicode characters in index name.\n",
    "        try:\n",
    "            rs.index.name = rs.index.name.decode(\"unicode_escape\").encode(\n",
    "                \"ascii\", \"ignore\"\n",
    "            )\n",
    "        except AttributeError:\n",
    "            # Python 3 string has no decode method.\n",
    "            rs.index.name = rs.index.name.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "        return rs\n",
    "\n",
    "    def _output_error(self, response: requests.Response) -> bool:\n",
    "        \"\"\"\n",
    "        Handle HTTP error responses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        response : requests.Response\n",
    "            Response object to check for errors\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if error should stop retry loop, False otherwise\n",
    "        \"\"\"\n",
    "        # Override in subclasses for specific error handling\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1e1e3-8f6c-46e1-8190-373b732d2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_version = \"3.0\"\n",
    "_BASE_URL = f\"https://api.e-stat.go.jp/rest/{_version}/app/json\"\n",
    "ATTR_DICT = {\n",
    "    \"value\": \"値\", \n",
    "    \"code\": \"コード\", \n",
    "    \"name\": \"\", \n",
    "    \"level\": \"階層レベル\", \n",
    "    \"tab\": \"表章項目\", \n",
    "    \"cat\": \"分類\", \n",
    "    \"area\": \"地域\", \n",
    "    \"time\": \"時間軸\", \n",
    "    \"unit\": \"単位\", \n",
    "    \"parentCode\": \"親コード\", \n",
    "    \"addInf\": \"追加情報\", \n",
    "    \"annotation\": \"注釈記号\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b0f02-bd96-4956-a7b9-d3cf09c5d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _eStatReader(_BaseReader):\n",
    "    \"\"\"\n",
    "    Base class for eStat API readers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : Optional[str], default None\n",
    "        取得したアプリケーションIDを指定して下さい。\n",
    "        eStat API key. If None, will try to get from environment variables\n",
    "        in the following order:\n",
    "        E_STAT_APPLICATION_ID, ESTAT_APPLICATION_ID,\n",
    "        E_STAT_APP_ID, ESTAT_APP_ID,\n",
    "        E_STAT_APPID, ESTAT_APPID,\n",
    "        E_STAT_API_KEY, ESTAT_API_KEY\n",
    "    lang : str, default \"J\"\n",
    "        取得するデータの言語を 以下のいずれかを指定して下さい。\n",
    "        ・J：日本語 (省略値)\n",
    "        ・E：英語\n",
    "        Language for retrieved data. Either \"J\" (Japanese) or \"E\" (English).\n",
    "    explanationGetFlg : Optional[str], default None\n",
    "        統計表及び、提供統計、提供分類、各事項の解説を取得するか否かを以下のいずれかから指定して下さい。\n",
    "        ・Y：取得する (省略値)\n",
    "        ・N：取得しない\n",
    "        Flag for getting explanation data (\"Y\" or \"N\").\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request.\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries.\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds.\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used.\n",
    "    dotenv_path : Optional[str], default None\n",
    "        Path to .env file for loading environment variables.\n",
    "        If None, will look for .estat_env, .env_estat, or .env in the current directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        lang: Optional[str] = None,\n",
    "        explanationGetFlg: Optional[str] = None,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "        dotenv_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            retry_count=retry_count,\n",
    "            pause=pause,\n",
    "            timeout=timeout,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        # Try to get API key from various sources\n",
    "        if api_key is None:\n",
    "            api_key = self._get_api_key_from_env(dotenv_path)\n",
    "                \n",
    "        if not api_key or not isinstance(api_key, str):\n",
    "            raise ValueError(\n",
    "                \"The e-Stat Application ID must be provided either \"\n",
    "                \"through the api_key variable or through one of the \"\n",
    "                \"following environment variables: \"\n",
    "                \"E_STAT_APPLICATION_ID, ESTAT_APPLICATION_ID, \"\n",
    "                \"E_STAT_APP_ID, ESTAT_APP_ID, \"\n",
    "                \"E_STAT_APPID, ESTAT_APPID, \"\n",
    "                \"E_STAT_API_KEY, ESTAT_API_KEY\"\n",
    "            )\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.explanationGetFlg = explanationGetFlg\n",
    "        self.lang = lang\n",
    "\n",
    "    def _get_api_key_from_env(self, dotenv_path: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get API key from environment variables or .env files.\n",
    "        First tries dotenv files, then falls back to environment variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dotenv_path : Optional[str]\n",
    "            Path to specific .env file to load. If None, tries default files.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Optional[str]\n",
    "            API key if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Environment variable names to try in order\n",
    "        env_vars = [\n",
    "            \"E_STAT_APPLICATION_ID\",\n",
    "            \"ESTAT_APPLICATION_ID\", \n",
    "            \"E_STAT_APP_ID\",\n",
    "            \"ESTAT_APP_ID\",\n",
    "            \"E_STAT_APPID\",\n",
    "            \"ESTAT_APPID\",\n",
    "            \"E_STAT_API_KEY\",\n",
    "            \"ESTAT_API_KEY\"\n",
    "        ]\n",
    "        \n",
    "        # First try dotenv files if available\n",
    "        if DOTENV_AVAILABLE:\n",
    "            if dotenv_path:\n",
    "                # If specific dotenv path is provided\n",
    "                if Path(dotenv_path).exists():\n",
    "                    load_dotenv(dotenv_path)\n",
    "                    # Try all environment variables after loading the specified file\n",
    "                    for var_name in env_vars:\n",
    "                        api_key = os.getenv(var_name)\n",
    "                        if api_key:\n",
    "                            return api_key\n",
    "            else:\n",
    "                # Try default .env files\n",
    "                env_files = [\".estat_env\", \".env_estat\", \".env\"]\n",
    "                \n",
    "                for env_file in env_files:\n",
    "                    if Path(env_file).exists():\n",
    "                        load_dotenv(env_file)\n",
    "                        # Try all environment variables after loading each file\n",
    "                        for var_name in env_vars:\n",
    "                            api_key = os.getenv(var_name)\n",
    "                            if api_key:\n",
    "                                return api_key\n",
    "        \n",
    "        # Fallback to regular environment variables\n",
    "        for var_name in env_vars:\n",
    "            api_key = os.getenv(var_name)\n",
    "            if api_key:\n",
    "                return api_key\n",
    "                \n",
    "        return None\n",
    "\n",
    "    def get_url(self, path: str = \"getStatsData\") -> str:\n",
    "        \"\"\"\n",
    "        Get API URL for specified path.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, default \"getStatsList\"\n",
    "            API endpoint path\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Complete API URL\n",
    "        \"\"\"\n",
    "        valid_paths = [\"getStatsList\", \"getDataCatalog\", \"getMetaInfo\", \"getStatsData\"]\n",
    "        if path not in valid_paths:\n",
    "            path = \"getStatsData\"\n",
    "            print(\n",
    "                f\"pathは{', '.join(valid_paths)}で指定します。pathをgetStatsDataに置換しました。\"\n",
    "            )\n",
    "        return f\"{_BASE_URL}/{path}?\"\n",
    "\n",
    "    def colname_to_japanese(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert column names to Japanese using non-destructive assign pattern.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Input DataFrame\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with Japanese column names\n",
    "        \"\"\"\n",
    "        def convert_column_name(col: str) -> str:\n",
    "            for k, v in ATTR_DICT.items():\n",
    "                col = col.replace(k, v)\n",
    "            return col\n",
    "        \n",
    "        new_columns = {col: convert_column_name(col) for col in df.columns}\n",
    "        return df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea5e42-8811-4413-add1-a8ed58c5f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchy_dataframe(metainfo: Dict[str, Any], cat_key: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a hierarchical DataFrame based on metadata information.\n",
    "    \n",
    "    This function creates a DataFrame where each row represents a bottom-level node\n",
    "    in the hierarchy, with columns for each hierarchical level containing \n",
    "    \"code_name\" format values. Missing intermediate levels are forward-filled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metainfo : Dict[str, Any]\n",
    "        Metadata information containing hierarchical data with @code, @name, \n",
    "        @level, and @parentCode fields\n",
    "    cat_key : int\n",
    "        Target category key index\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Hierarchical DataFrame with bottom-level nodes as rows and \n",
    "        hierarchical levels as columns\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> hierarchy_df = create_hierarchy_dataframe(metainfo, 0)\n",
    "    >>> print(hierarchy_df.head())\n",
    "    \"\"\"\n",
    "    # Extract target category metadata\n",
    "    cat_meta = metainfo[\"GET_META_INFO\"][\"METADATA_INF\"][\"CLASS_INF\"][\"CLASS_OBJ\"][cat_key]\n",
    "    meta_name = cat_meta[\"@name\"]\n",
    "    meta_cls_df = pd.DataFrame(cat_meta[\"CLASS\"]).assign(\n",
    "        **{\"@level\": lambda df: df[\"@level\"].astype(int)}\n",
    "    )\n",
    "    \n",
    "    # Create set of parent codes for identifying leaf nodes\n",
    "    parent_codes = {\n",
    "        row.get(\"@parentCode\") \n",
    "        for _, row in meta_cls_df.iterrows() \n",
    "        if row.get(\"@parentCode\") and str(row.get(\"@parentCode\")).strip()\n",
    "    }\n",
    "    \n",
    "    # Create code-to-record mapping\n",
    "    code_to_record = {row[\"@code\"]: row for _, row in meta_cls_df.iterrows()}\n",
    "\n",
    "    def _get_ancestry_chain(meta_record: Dict[str, Any]) -> Dict[int, str]:\n",
    "        \"\"\"\n",
    "        Get ancestry chain for a metadata record.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        meta_record : Dict[str, Any]\n",
    "            Metadata record with @code, @name, @level, @parentCode fields\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[int, str]\n",
    "            Dictionary mapping level to code for the ancestry chain\n",
    "        \"\"\"\n",
    "        chain = {}\n",
    "        current_record = meta_record\n",
    "        \n",
    "        while current_record is not None:\n",
    "            level = current_record[\"@level\"]\n",
    "            chain[level] = current_record[\"@code\"]\n",
    "            parent_code = current_record.get(\"@parentCode\")\n",
    "            \n",
    "            if not parent_code or parent_code not in code_to_record:\n",
    "                break\n",
    "                \n",
    "            current_record = code_to_record[parent_code]\n",
    "        \n",
    "        return chain\n",
    "\n",
    "    # Process leaf nodes only\n",
    "    max_level = meta_cls_df[\"@level\"].max()\n",
    "    chain_rows = []\n",
    "    \n",
    "    for _, row in meta_cls_df.iterrows():\n",
    "        # Skip parent nodes\n",
    "        if row[\"@code\"] in parent_codes:\n",
    "            continue\n",
    "\n",
    "        node_level = row[\"@level\"]\n",
    "        ancestry = _get_ancestry_chain(row)\n",
    "        row_chain = {}\n",
    "        last_code = None\n",
    "        \n",
    "        # Build hierarchy with forward fill\n",
    "        for level in range(1, max_level + 1):\n",
    "            col = f\"level{level}\"\n",
    "            if level <= node_level:\n",
    "                if level in ancestry:\n",
    "                    last_code = ancestry[level]\n",
    "                    row_chain[col] = ancestry[level]\n",
    "                else:\n",
    "                    row_chain[col] = last_code  # Forward fill\n",
    "            else:\n",
    "                row_chain[col] = None\n",
    "                \n",
    "        chain_rows.append(row_chain)\n",
    "\n",
    "    hierarchy_df = pd.DataFrame(chain_rows)\n",
    "\n",
    "    # Merge with names to create \"code_name\" format\n",
    "    for level in range(1, max_level + 1):\n",
    "        level_col = f\"level{level}\"\n",
    "        name_col = f\"{meta_name}階層{level}\"\n",
    "        \n",
    "        name_df = meta_cls_df[[\"@code\", \"@name\"]].assign(\n",
    "            **{\n",
    "                level_col: meta_cls_df[\"@code\"],\n",
    "                name_col: meta_cls_df[\"@code\"] + \"_\" + meta_cls_df[\"@name\"]\n",
    "            }\n",
    "        )[[\"@code\", name_col]].rename(columns={\"@code\": level_col})\n",
    "        \n",
    "        hierarchy_df = hierarchy_df.merge(name_df, on=level_col, how=\"left\")\n",
    "\n",
    "    # Apply forward fill to both code and name columns\n",
    "    level_cols = [f\"level{level}\" for level in range(1, max_level + 1)]\n",
    "    hierarchy_cols = [f\"{meta_name}階層{level}\" for level in range(1, max_level + 1)]\n",
    "    \n",
    "    hierarchy_df[level_cols] = hierarchy_df[level_cols].fillna(method=\"ffill\", axis=1)\n",
    "    hierarchy_df[hierarchy_cols] = hierarchy_df[hierarchy_cols].fillna(method=\"ffill\", axis=1)\n",
    "\n",
    "    return hierarchy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568967bf-9228-40fb-bf5b-555cf8e3c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaInfoReader(_eStatReader):\n",
    "    \"\"\"\n",
    "    Reader for e-Stat meta infomation API.\n",
    "    メタ情報取得 API\n",
    "    URL: https://www.e-stat.go.jp/api/api-info/e-stat-manual3-0#api_3_3\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : str\n",
    "        e-Stat application ID (appId)\n",
    "    statsDataId : Union[str, int]\n",
    "        Statistics data ID\n",
    "        「統計表情報取得」で得られる統計表IDです。\n",
    "    name_or_id : str, default \"name\"\n",
    "        Whether to use \"name\" or \"id\" for column naming\n",
    "    lvhierarchy : bool, default False\n",
    "        Whether to create hierarchy levels\n",
    "    lvfillna : bool, default False\n",
    "        Whether to fill NA values in hierarchy levels\n",
    "    explanationGetFlg : Optional[str], default None\n",
    "        Flag for getting explanation data (\"Y\" or \"N\")\n",
    "        統計表及び、提供統計、提供分類、各事項の解説を取得するか否かを以下のいずれかから指定して下さい。\n",
    "        ・Y：取得する (省略値)\n",
    "        ・N：取得しない\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used\n",
    "    dotenv_path : Optional[str], default None\n",
    "        Path to .env file for loading environment variables.\n",
    "        If None, will look for .estat_env, .env_estat, or .env in the current directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        statsDataId: Union[str, int],\n",
    "        name_or_id: str = \"name\",\n",
    "        lvhierarchy: bool = False,\n",
    "        lvfillna: bool = False,\n",
    "        explanationGetFlg: Optional[str] = None,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "        dotenv_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            api_key=api_key,\n",
    "            explanationGetFlg=explanationGetFlg,\n",
    "            retry_count=retry_count,\n",
    "            pause=pause,\n",
    "            timeout=timeout,\n",
    "            session=session,\n",
    "            dotenv_path=dotenv_path,\n",
    "        )\n",
    "\n",
    "        self.statsDataId = statsDataId\n",
    "        self.name_or_id = name_or_id\n",
    "        self.lvhierarchy = lvhierarchy\n",
    "        self.lvfillna = lvfillna\n",
    "\n",
    "    @property\n",
    "    def url(self) -> str:\n",
    "        \"\"\"API URL for getMetaInfo.\"\"\"\n",
    "        return self.get_url(\"getMetaInfo\")\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Parameters to use in API calls.\"\"\"\n",
    "        pdict = {\"appId\": self.api_key}\n",
    "\n",
    "        if isinstance(self.statsDataId, (str, int)):\n",
    "            pdict[\"statsDataId\"] = self.statsDataId\n",
    "        if self.explanationGetFlg in [\"Y\", \"N\"]:\n",
    "            pdict[\"explanationGetFlg\"] = self.explanationGetFlg\n",
    "\n",
    "        return pdict\n",
    "    \n",
    "    def read(self) -> List[Union[pd.DataFrame, List[pd.DataFrame]]]:\n",
    "        \"\"\"\n",
    "        Read data from connector and return list of DataFrames.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Union[pd.DataFrame, List[pd.DataFrame]]]\n",
    "            List of DataFrames for each CLASS_OBJ. If lvhierarchy=True,\n",
    "            returns list of [class_df, hierarchy_df] pairs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.read_class_objects()\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def read_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"Read data from connector and return as raw JSON.\"\"\"\n",
    "        try:\n",
    "            response = self._get_response(self.url, params=self.params)\n",
    "            json_data = response.json()\n",
    "            \n",
    "            # Store response metadata as instance attributes\n",
    "            meta_info = json_data.get(\"GET_META_INFO\", {})\n",
    "            self._store_metadata_attributes(meta_info)\n",
    "            \n",
    "            return json_data\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "\n",
    "    def read_class_objects(self) -> List[Union[pd.DataFrame, List[pd.DataFrame]]]:\n",
    "        \"\"\"\n",
    "        Read and process CLASS_OBJ data into DataFrames.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Union[pd.DataFrame, List[pd.DataFrame]]]\n",
    "            List of processed DataFrames\n",
    "        \"\"\"\n",
    "        response = self._get_response(self.url, params=self.params)\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Store response metadata as instance attributes\n",
    "        meta_info = json_data.get(\"GET_META_INFO\", {})\n",
    "        self._store_metadata_attributes(meta_info)\n",
    "        \n",
    "        # Get class objects\n",
    "        class_obj = meta_info.get(\"METADATA_INF\", {}).get(\"CLASS_INF\", {}).get(\"CLASS_OBJ\", [])\n",
    "        \n",
    "        if not isinstance(class_obj, list):\n",
    "            print(\"CLASS_OBJはlist型ではありません。\")\n",
    "            return []\n",
    "        \n",
    "        result_dfs = {}\n",
    "        \n",
    "        for i, co in enumerate(class_obj):\n",
    "            class_data = co.get(\"CLASS\")\n",
    "            class_df = self._create_class_dataframe(class_data, co)\n",
    "            \n",
    "            if class_df is None:\n",
    "                continue\n",
    "                \n",
    "            # クラス名を取得（複数の方法で試行）\n",
    "            class_name = co.get(\"@name\") or co.get(\"@id\") or f\"class_{i}\"\n",
    "            \n",
    "            # Check if hierarchy processing is needed\n",
    "            is_hierarchy = self.lvhierarchy and len(class_df[\"level\"].unique()) > 1\n",
    "            \n",
    "            if is_hierarchy:\n",
    "                # Use the external function to create hierarchy\n",
    "                hierarchy_df = create_hierarchy_dataframe(json_data, i)\n",
    "                result_dfs[class_name] = [class_df, hierarchy_df]\n",
    "            else:\n",
    "                result_dfs[class_name] = class_df\n",
    "        \n",
    "        return result_dfs\n",
    "\n",
    "    def _store_metadata_attributes(self, meta_info: Dict[str, Any]) -> None:\n",
    "        \"\"\"Store metadata attributes as instance variables.\"\"\"\n",
    "        result = meta_info.get(\"RESULT\", {})\n",
    "        self.STATUS = result.get(\"STATUS\")\n",
    "        self.ERROR_MSG = result.get(\"ERROR_MSG\")\n",
    "        self.DATE = result.get(\"DATE\")\n",
    "\n",
    "        parameter = meta_info.get(\"PARAMETER\", {})\n",
    "        self.LANG = parameter.get(\"LANG\")\n",
    "        self.DATA_FORMAT = parameter.get(\"DATA_FORMAT\")\n",
    "\n",
    "        # Store table information\n",
    "        table_inf = meta_info.get(\"METADATA_INF\", {}).get(\"TABLE_INF\", {})\n",
    "        self.TABLE_INF = table_inf\n",
    "        \n",
    "        # Store individual table attributes\n",
    "        table_attributes = [\n",
    "            \"STAT_NAME\", \"GOV_ORG\", \"STATISTICS_NAME\", \"TITLE\", \"CYCLE\",\n",
    "            \"SURVEY_DATE\", \"OPEN_DATE\", \"SMALL_AREA\", \"COLLECT_AREA\",\n",
    "            \"MAIN_CATEGORY\", \"SUB_CATEGORY\", \"OVERALL_TOTAL_NUMBER\",\n",
    "            \"UPDATED_DATE\", \"STATISTICS_NAME_SPEC\", \"TABULATION_SUB_CATEGORY1\",\n",
    "            \"DESCRIPTION\", \"TITLE_SPEC\"\n",
    "        ]\n",
    "        \n",
    "        for attr in table_attributes:\n",
    "            setattr(self, attr, table_inf.get(attr))\n",
    "\n",
    "    def _create_class_dataframe(self, class_data: Union[List[Dict[str, Any]], Dict[str, Any]], class_obj: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create DataFrame from class data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        class_data : Union[List[Dict[str, Any]], Dict[str, Any]]\n",
    "            Class data from API response (can be list or dict)\n",
    "        class_obj : Dict[str, Any]\n",
    "            Class object metadata\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Optional[pd.DataFrame]\n",
    "            DataFrame created from class data, or None if failed\n",
    "        \"\"\"\n",
    "        if not class_data:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Handle different types of class_data\n",
    "            if isinstance(class_data, list):\n",
    "                df = pd.DataFrame(class_data)\n",
    "            elif isinstance(class_data, dict):\n",
    "                df = pd.DataFrame(pd.Series(class_data)).T\n",
    "            else:\n",
    "                print(f\"CLASS_INF>CLASS_OBJ>CLASSの型: {type(class_data)}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert level to int if exists, handle empty strings\n",
    "            if \"@level\" in df.columns:\n",
    "                # Replace empty strings with NaN, then convert to nullable int\n",
    "                df = df.assign(**{\n",
    "                    \"level\": lambda d: pd.to_numeric(d[\"@level\"].replace(\"\", pd.NA), errors=\"coerce\").astype(\"Int64\")\n",
    "                })\n",
    "            \n",
    "            # Rename columns with class name prefix\n",
    "            class_name = class_obj.get(\"@name\", \"unknown\")\n",
    "            df = df.rename(columns=lambda col: f\"{class_name}{col.lstrip('@')}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame for class {class_obj.get('@id', 'unknown')}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf047172-add8-4d8b-a6f7-08cb81b02e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac092d2-4435-4983-8c25-da1fd204ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colname_to_japanese(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert column names to Japanese using non-destructive assign pattern.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with Japanese column names\n",
    "    \"\"\"\n",
    "    def convert_column_name(col: str) -> str:\n",
    "        for k, v in ATTR_DICT.items():\n",
    "            col = col.replace(k, v)\n",
    "        return col\n",
    "    \n",
    "    new_columns = {col: convert_column_name(col) for col in df.columns}\n",
    "    return df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061c4bb-911b-4c06-b87f-baec4aedae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_class_dataframe(\n",
    "    class_data: Union[List[Dict], Dict], \n",
    "    class_obj: Dict[str, Any],\n",
    "    name_or_id=\"name\"\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create DataFrame from class data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    class_data : Union[List[Dict], Dict]\n",
    "        Class data from API response\n",
    "    class_obj : Dict[str, Any]\n",
    "        Class object metadata\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[pd.DataFrame]\n",
    "        Processed class DataFrame or None if invalid\n",
    "    \"\"\"\n",
    "    if isinstance(class_data, list):\n",
    "        class_df = pd.DataFrame(class_data)\n",
    "    elif isinstance(class_data, dict):\n",
    "        class_df = pd.DataFrame(pd.Series(class_data)).T\n",
    "    else:\n",
    "        print(f\"{class_obj['@name']}はlist型でもdict型でもありません。\")\n",
    "        return None\n",
    "\n",
    "    # Clean column names\n",
    "    class_df = class_df.assign(**{\n",
    "        col.lstrip(\"@\"): class_df[col] for col in class_df.columns\n",
    "    }).drop(columns=class_df.columns.tolist())\n",
    "\n",
    "    # Apply naming convention\n",
    "    if name_or_id == \"name\":\n",
    "        class_df = class_df.assign(**{\n",
    "            f\"{class_obj['@name']}{col}\": class_df[col] \n",
    "            for col in class_df.columns\n",
    "        }).drop(columns=class_df.columns.tolist())\n",
    "        class_df = colname_to_japanese(class_df)\n",
    "    else:\n",
    "        class_df = class_df.assign(**{\n",
    "            f\"{class_obj['@id']}_{col}\": class_df[col] \n",
    "            for col in class_df.columns\n",
    "        }).drop(columns=class_df.columns.tolist())\n",
    "\n",
    "    return class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4a5a3-4361-443a-8499-447e08ed5e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc05709-f689-4caa-936b-4f7b1776ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "appId = os.getenv(\"ESTAT_APP_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7ef0b-9ba1-4175-98b8-e0beea89a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDataId = \"0002070010\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c42d6e-8683-47d3-99d0-de3cc0ab2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo = MetaInfoReader(api_key=appId, statsDataId=statsDataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821a54d-63e5-4e21-b5a5-40ec83babaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.url, metainfo.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4e8d7-ed9b-42f0-8bf5-c576db7149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = metainfo._get_response(metainfo.url, params=metainfo.params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba699c-ac35-4c3d-8366-5725666bd598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a76b27-afc9-4937-87cf-32e2f648b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo._store_metadata_attributes(meta_json[\"GET_META_INFO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7b8a3-5eef-4de8-8aca-028907412c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348e3b6-7875-4650-bfed-be4d80634b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj = meta_json[\"GET_META_INFO\"].get(\"METADATA_INF\", {}).get(\"CLASS_INF\", {}).get(\"CLASS_OBJ\", [])\n",
    "class_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfe68e-da24-4d2c-b39c-94c9336d2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs = metainfo.read_class_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bd54e-bd75-4d3b-a618-5e5bd34567b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544a769-8742-49fd-8c46-854f9890946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs['世帯主の年齢階級']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454512f8-9cb6-410c-bb58-8cfab12df469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
