{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513f9dc-ea81-4a62-b70c-e3428be6e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Any, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bad63b-056c-4284-bb90-f4abbbf3bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteDataError(IOError):\n",
    "    pass\n",
    "\n",
    "def _init_session(session):\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    else:\n",
    "        if not isinstance(session, requests.Session):\n",
    "            raise TypeError(\"session must be a request.Session\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adae012-1983-4115-9730-85f678a531fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseReader:\n",
    "    \"\"\"\n",
    "    Base class for data readers with retry and session management.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request.\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries.\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds.\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "    ) -> None:\n",
    "        if not isinstance(retry_count, int) or retry_count < 0:\n",
    "            raise ValueError(\"'retry_count' must be integer larger than 0\")\n",
    "        if not isinstance(pause, (int, float)) or pause < 0:\n",
    "            raise ValueError(\"'pause' must be a positive number\")\n",
    "        if not isinstance(timeout, int) or timeout <= 0:\n",
    "            raise ValueError(\"'timeout' must be a positive integer\")\n",
    "        \n",
    "        self.retry_count = retry_count\n",
    "        self.pause = pause\n",
    "        self.timeout = timeout\n",
    "        self.pause_multiplier = 1\n",
    "        self.session = _init_session(session)\n",
    "        self.headers: Optional[Dict[str, str]] = None\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close network session.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "    @property\n",
    "    def url(self) -> str:\n",
    "        \"\"\"API URL - must be overridden in subclass.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Parameters to use in API calls.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def read(self) -> pd.DataFrame:\n",
    "        \"\"\"Read data from connector.\"\"\"\n",
    "        try:\n",
    "            return self._read_one_data(self.url, self.params)\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def read_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"Read data from connector and return as raw JSON.\"\"\"\n",
    "        try:\n",
    "            response = self._get_response(self.url, params=self.params)\n",
    "            return response.json()\n",
    "        finally:\n",
    "            self.close()\n",
    "    \n",
    "    def _read_one_data(self, url: str, params: Optional[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"Read one data from specified URL.\"\"\"\n",
    "        out = self._get_response(url, params=params).json()\n",
    "        return self._read_lines(out)\n",
    "\n",
    "    def _get_response(\n",
    "        self, \n",
    "        url: str, \n",
    "        params: Optional[Dict[str, Any]] = None, \n",
    "        headers: Optional[Dict[str, str]] = None\n",
    "    ) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Send raw HTTP request to get requests.Response from the specified url.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            Target URL\n",
    "        params : Optional[Dict[str, Any]]\n",
    "            Parameters passed to the URL\n",
    "        headers : Optional[Dict[str, str]]\n",
    "            Headers for the request\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        requests.Response\n",
    "            Response object from the HTTP request\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        RemoteDataError\n",
    "            If unable to retrieve data after all retry attempts\n",
    "        \"\"\"\n",
    "        headers = headers or self.headers\n",
    "        pause = self.pause\n",
    "        last_response_text = \"\"\n",
    "        \n",
    "        for _ in range(self.retry_count + 1):\n",
    "            response = self.session.get(\n",
    "                url, params=params, headers=headers, timeout=self.timeout\n",
    "            )\n",
    "            if response.status_code == requests.codes.ok:\n",
    "                return response\n",
    "\n",
    "            if response.encoding:\n",
    "                last_response_text = response.text.encode(response.encoding)\n",
    "            time.sleep(pause)\n",
    "\n",
    "            # Increase time between subsequent requests, per subclass.\n",
    "            pause *= self.pause_multiplier\n",
    "\n",
    "            # If our output error function returns True, exit the loop.\n",
    "            if self._output_error(response):\n",
    "                break\n",
    "\n",
    "        # If we reach here, we have exhausted all retries.\n",
    "        if params is not None and len(params) > 0:\n",
    "            url = url + \"?\" + urllib.parse.urlencode(query=params)\n",
    "        msg = f\"Unable to read URL: {url}\"\n",
    "        if last_response_text:\n",
    "            msg += f\"\\nResponse Text:\\n{last_response_text}\"\n",
    "\n",
    "        raise RemoteDataError(msg)\n",
    "\n",
    "    def _read_lines(self, out: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process JSON response into DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        out : Dict[str, Any]\n",
    "            JSON response data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        rs = pd.json_normalize(out, sep=\"_\")\n",
    "        # Remove blank space character in header names\n",
    "        rs = rs.assign(**{\n",
    "            col.strip(): rs[col] for col in rs.columns\n",
    "        }).drop(columns=rs.columns.tolist())\n",
    "\n",
    "        # Get rid of unicode characters in index name.\n",
    "        try:\n",
    "            rs.index.name = rs.index.name.decode(\"unicode_escape\").encode(\n",
    "                \"ascii\", \"ignore\"\n",
    "            )\n",
    "        except AttributeError:\n",
    "            # Python 3 string has no decode method.\n",
    "            rs.index.name = rs.index.name.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "        return rs\n",
    "\n",
    "    def _output_error(self, response: requests.Response) -> bool:\n",
    "        \"\"\"\n",
    "        Handle HTTP error responses.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        response : requests.Response\n",
    "            Response object to check for errors\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if error should stop retry loop, False otherwise\n",
    "        \"\"\"\n",
    "        # Override in subclasses for specific error handling\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1e1e3-8f6c-46e1-8190-373b732d2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_version = \"3.0\"\n",
    "_BASE_URL = f\"https://api.e-stat.go.jp/rest/{_version}/app/json\"\n",
    "ATTR_DICT = {\n",
    "    \"value\": \"値\", \n",
    "    \"code\": \"コード\", \n",
    "    \"name\": \"\", \n",
    "    \"level\": \"階層レベル\", \n",
    "    \"tab\": \"表章項目\", \n",
    "    \"cat\": \"分類\", \n",
    "    \"area\": \"地域\", \n",
    "    \"time\": \"時間軸\", \n",
    "    \"unit\": \"単位\", \n",
    "    \"parentCode\": \"親コード\", \n",
    "    \"addInf\": \"追加情報\", \n",
    "    \"annotation\": \"注釈記号\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b0f02-bd96-4956-a7b9-d3cf09c5d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _eStatReader(_BaseReader):\n",
    "    \"\"\"\n",
    "    Base class for eStat API readers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : Optional[str], default None\n",
    "        取得したアプリケーションIDを指定して下さい。\n",
    "        eStat API key. If None, will try to get from environment variables\n",
    "        in the following order:\n",
    "        E_STAT_APPLICATION_ID, ESTAT_APPLICATION_ID,\n",
    "        E_STAT_APP_ID, ESTAT_APP_ID,\n",
    "        E_STAT_APPID, ESTAT_APPID,\n",
    "        E_STAT_API_KEY, ESTAT_API_KEY\n",
    "    lang : str, default \"J\"\n",
    "        取得するデータの言語を 以下のいずれかを指定して下さい。\n",
    "        ・J：日本語 (省略値)\n",
    "        ・E：英語\n",
    "        Language for retrieved data. Either \"J\" (Japanese) or \"E\" (English).\n",
    "    explanationGetFlg : Optional[str], default None\n",
    "        統計表及び、提供統計、提供分類、各事項の解説を取得するか否かを以下のいずれかから指定して下さい。\n",
    "        ・Y：取得する (省略値)\n",
    "        ・N：取得しない\n",
    "        Flag for getting explanation data (\"Y\" or \"N\").\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request.\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries.\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds.\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used.\n",
    "    dotenv_path : Optional[str], default None\n",
    "        Path to .env file for loading environment variables.\n",
    "        If None, will look for .estat_env, .env_estat, or .env in the current directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        lang: Optional[str] = None,\n",
    "        explanationGetFlg: Optional[str] = None,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "        dotenv_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            retry_count=retry_count,\n",
    "            pause=pause,\n",
    "            timeout=timeout,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        # Try to get API key from various sources\n",
    "        if api_key is None:\n",
    "            api_key = self._get_api_key_from_env(dotenv_path)\n",
    "                \n",
    "        if not api_key or not isinstance(api_key, str):\n",
    "            raise ValueError(\n",
    "                \"The e-Stat Application ID must be provided either \"\n",
    "                \"through the api_key variable or through one of the \"\n",
    "                \"following environment variables: \"\n",
    "                \"E_STAT_APPLICATION_ID, ESTAT_APPLICATION_ID, \"\n",
    "                \"E_STAT_APP_ID, ESTAT_APP_ID, \"\n",
    "                \"E_STAT_APPID, ESTAT_APPID, \"\n",
    "                \"E_STAT_API_KEY, ESTAT_API_KEY\"\n",
    "            )\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.explanationGetFlg = explanationGetFlg\n",
    "        self.lang = lang\n",
    "\n",
    "    def _get_api_key_from_env(self, dotenv_path: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get API key from environment variables or .env files.\n",
    "        First tries dotenv files, then falls back to environment variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dotenv_path : Optional[str]\n",
    "            Path to specific .env file to load. If None, tries default files.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Optional[str]\n",
    "            API key if found, None otherwise\n",
    "        \"\"\"\n",
    "        # Environment variable names to try in order\n",
    "        env_vars = [\n",
    "            \"E_STAT_APPLICATION_ID\",\n",
    "            \"ESTAT_APPLICATION_ID\", \n",
    "            \"E_STAT_APP_ID\",\n",
    "            \"ESTAT_APP_ID\",\n",
    "            \"E_STAT_APPID\",\n",
    "            \"ESTAT_APPID\",\n",
    "            \"E_STAT_API_KEY\",\n",
    "            \"ESTAT_API_KEY\"\n",
    "        ]\n",
    "        \n",
    "        # First try dotenv files if available\n",
    "        if DOTENV_AVAILABLE:\n",
    "            if dotenv_path:\n",
    "                # If specific dotenv path is provided\n",
    "                if Path(dotenv_path).exists():\n",
    "                    load_dotenv(dotenv_path)\n",
    "                    # Try all environment variables after loading the specified file\n",
    "                    for var_name in env_vars:\n",
    "                        api_key = os.getenv(var_name)\n",
    "                        if api_key:\n",
    "                            return api_key\n",
    "            else:\n",
    "                # Try default .env files\n",
    "                env_files = [\".estat_env\", \".env_estat\", \".env\"]\n",
    "                \n",
    "                for env_file in env_files:\n",
    "                    if Path(env_file).exists():\n",
    "                        load_dotenv(env_file)\n",
    "                        # Try all environment variables after loading each file\n",
    "                        for var_name in env_vars:\n",
    "                            api_key = os.getenv(var_name)\n",
    "                            if api_key:\n",
    "                                return api_key\n",
    "        \n",
    "        # Fallback to regular environment variables\n",
    "        for var_name in env_vars:\n",
    "            api_key = os.getenv(var_name)\n",
    "            if api_key:\n",
    "                return api_key\n",
    "                \n",
    "        return None\n",
    "\n",
    "    def get_url(self, path: str = \"getStatsData\") -> str:\n",
    "        \"\"\"\n",
    "        Get API URL for specified path.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, default \"getStatsList\"\n",
    "            API endpoint path\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Complete API URL\n",
    "        \"\"\"\n",
    "        valid_paths = [\"getStatsList\", \"getDataCatalog\", \"getMetaInfo\", \"getStatsData\"]\n",
    "        if path not in valid_paths:\n",
    "            path = \"getStatsData\"\n",
    "            print(\n",
    "                f\"pathは{', '.join(valid_paths)}で指定します。pathをgetStatsDataに置換しました。\"\n",
    "            )\n",
    "        return f\"{_BASE_URL}/{path}?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94b5fb-6d58-4669-af98-4c057d57d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 列名を日本語に変換\n",
    "def colname_to_japanese(value: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 英語と日本語の対応\n",
    "    attrdict = {\"value\": \"値\", \"code\": \"コード\", \"name\": \"\", \"level\": \"階層レベル\", \n",
    "        \"unit\": \"単位\", \"parentCode\": \"親コード\", \"addInf\": \"追加情報\", \"tab\": \"表章項目\", \n",
    "        \"cat\": \"分類\", \"area\": \"地域\", \"time\": \"時間軸\", \"annotation\": \"注釈記号\"  \n",
    "    }\n",
    "    def _convert(c):\n",
    "        for k, v in attrdict.items():\n",
    "            if k in c:\n",
    "                return c.replace(k, v)\n",
    "        return c\n",
    "    return value.rename(columns=_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568967bf-9228-40fb-bf5b-555cf8e3c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaInfoReader(_eStatReader):\n",
    "    \"\"\"\n",
    "    Reader for e-Stat meta infomation API.\n",
    "    メタ情報取得 API\n",
    "    URL: https://www.e-stat.go.jp/api/api-info/e-stat-manual3-0#api_3_3\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : str\n",
    "        e-Stat application ID (appId)\n",
    "    statsDataId : Union[str, int]\n",
    "        Statistics data ID\n",
    "        「統計表情報取得」で得られる統計表IDです。\n",
    "    prefix_colname_with_classname: bool, default True\n",
    "        Whether to prefix column names with class names\n",
    "    has_lv_hierarchy : bool, default False\n",
    "        Whether to create hierarchy levels\n",
    "    use_fillna_lv_hierarchy : bool, default False\n",
    "        Whether to fill NA values in hierarchy levels\n",
    "    lang : Optional[str], default None\n",
    "        Language for retrieved data. Either \"J\" (Japanese) or \"E\" (English).\n",
    "        取得するデータの言語を 以下のいずれかを指定して下さい。\n",
    "        ・J：日本語 (省略値)\n",
    "        ・E：英語\n",
    "    explanationGetFlg : Optional[str], default None\n",
    "        Flag for getting explanation data (\"Y\" or \"N\")\n",
    "        統計表及び、提供統計、提供分類、各事項の解説を取得するか否かを以下のいずれかから指定して下さい。\n",
    "        ・Y：取得する (省略値)\n",
    "        ・N：取得しない\n",
    "    retry_count : int, default 3\n",
    "        Number of times to retry query request\n",
    "    pause : float, default 0.1\n",
    "        Time, in seconds, of the pause between retries\n",
    "    timeout : int, default 30\n",
    "        Request timeout in seconds\n",
    "    session : Optional[requests.Session], default None\n",
    "        requests.sessions.Session instance to be used\n",
    "    dotenv_path : Optional[str], default None\n",
    "        Path to .env file for loading environment variables.\n",
    "        If None, will look for .estat_env, .env_estat, or .env in the current directory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        statsDataId: Union[str, int],\n",
    "        prefix_colname_with_classname: bool = True,\n",
    "        has_lv_hierarchy: bool = False,\n",
    "        use_fillna_lv_hierarchy: bool = True,\n",
    "        lang: Optional[str] = None,\n",
    "        explanationGetFlg: Optional[str] = None,\n",
    "        retry_count: int = 3,\n",
    "        pause: float = 0.1,\n",
    "        timeout: int = 30,\n",
    "        session: Optional[requests.Session] = None,\n",
    "        dotenv_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            api_key=api_key,\n",
    "            lang=lang,\n",
    "            explanationGetFlg=explanationGetFlg,\n",
    "            retry_count=retry_count,\n",
    "            pause=pause,\n",
    "            timeout=timeout,\n",
    "            session=session,\n",
    "            dotenv_path=dotenv_path,\n",
    "        )\n",
    "\n",
    "        self.statsDataId = statsDataId\n",
    "        self.prefix_colname_with_classname = prefix_colname_with_classname\n",
    "        self.has_lv_hierarchy = has_lv_hierarchy\n",
    "        self.use_fillna_lv_hierarchy = use_fillna_lv_hierarchy\n",
    "\n",
    "    @property\n",
    "    def url(self) -> str:\n",
    "        \"\"\"API URL for getMetaInfo.\"\"\"\n",
    "        return self.get_url(\"getMetaInfo\")\n",
    "\n",
    "    @property\n",
    "    def params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Parameters to use in API calls.\"\"\"\n",
    "        pdict = {\"appId\": self.api_key}\n",
    "\n",
    "        if isinstance(self.statsDataId, (str, int)):\n",
    "            pdict[\"statsDataId\"] = self.statsDataId\n",
    "        if self.explanationGetFlg in [\"Y\", \"N\"]:\n",
    "            pdict[\"explanationGetFlg\"] = self.explanationGetFlg\n",
    "\n",
    "        return pdict\n",
    "    \n",
    "    def read(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read data from connector and return the DataFrame with the most rows.\n",
    "        Excludes DataFrames with 'id': 'time'.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with the most rows from all CLASS_OBJ DataFrames (excluding 'time')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result_dfs = self.read_class_obj_dfs()\n",
    "            \n",
    "            if not result_dfs:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Find the DataFrame with the most rows (excluding 'time')\n",
    "            max_rows = 0\n",
    "            largest_df = pd.DataFrame()\n",
    "            \n",
    "            for class_data in result_dfs:\n",
    "                # Skip if id is 'time'\n",
    "                if class_data[\"id\"] == 'time':\n",
    "                    continue\n",
    "                \n",
    "                df = class_data[\"meta_dataframe\"]\n",
    "                \n",
    "                if len(df) > max_rows:\n",
    "                    max_rows = len(df)\n",
    "                    largest_df = df\n",
    "            \n",
    "            return largest_df\n",
    "            \n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def read_json(self) -> Dict[str, Any]:\n",
    "        \"\"\"Read data from connector and return as raw JSON.\"\"\"\n",
    "        try:\n",
    "            response = self._get_response(self.url, params=self.params)\n",
    "            json_data = response.json()\n",
    "            \n",
    "            # Store response metadata as instance attributes\n",
    "            self._store_params_in_attrs(json_data)\n",
    "            \n",
    "            return json_data \n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def read_class_obj_dfs(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Read and process CLASS_OBJ data into DataFrames.\n",
    "        CLASS_OBJ dictionary's keys: ['@id', '@name', 'CLASS']\n",
    "        CLASS dictionary's keys: ['@code', '@name', '@level', '@unit']\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Dict[str, Any]]\n",
    "            List of dictionaries with keys: \"id\", \"name\", \"meta_dataframe\", \"hierarchy\"\n",
    "        \"\"\"\n",
    "        response = self._get_response(self.url, params=self.params)\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Store response metadata as instance attributes\n",
    "        self._store_params_in_attrs(json_data)\n",
    "        \n",
    "        # Get class objects\n",
    "        meta_info = json_data.get(\"GET_META_INFO\", {})\n",
    "        class_obj = meta_info.get(\"METADATA_INF\", {}).get(\"CLASS_INF\", {}).get(\"CLASS_OBJ\", [])\n",
    "        \n",
    "        if not isinstance(class_obj, list):\n",
    "            print(\"CLASS_OBJはlist型ではありません。\")\n",
    "            return []\n",
    "        \n",
    "        result_dfs = []\n",
    "        \n",
    "        for i, co in enumerate(class_obj):\n",
    "            # クラスIDを取得\n",
    "            class_id = co.get(\"@id\")\n",
    "            if not class_id:\n",
    "                print(f\"警告: クラスID（@id）が見つかりません。処理をスキップします。\")\n",
    "                continue\n",
    "            \n",
    "            # クラス名を取得（複数の方法で試行）\n",
    "            class_name = co.get(\"@name\")\n",
    "            if not class_name:\n",
    "                print(f\"警告: クラス名（@name）が見つかりません。処理をスキップします。クラスID: {class_id}\")\n",
    "                continue  # このクラスをスキップして次のクラスに進む\n",
    "            \n",
    "            class_data = co.get(\"CLASS\")\n",
    "            \n",
    "            # 列名変換前の生データフレームを作成\n",
    "            class_df_raw = self._create_class_dataframe_raw(class_data, co)\n",
    "            \n",
    "            if class_df_raw is None:\n",
    "                continue\n",
    "\n",
    "            # Check if hierarchy processing is needed (生データで判定)\n",
    "            hierarchy_df = None\n",
    "            if (self.has_lv_hierarchy and \n",
    "                \"@level\" in class_df_raw.columns and \n",
    "                len(class_df_raw[\"@level\"].unique()) > 1):\n",
    "                # Use the method to create hierarchy\n",
    "                hierarchy_df = self._create_hierarchy_dataframe(json_data, i)\n",
    "\n",
    "            # 列名変換を実行\n",
    "            class_df = self._apply_column_transformations(class_df_raw, class_name)\n",
    "\n",
    "            # Create result dictionary\n",
    "            result_dict = {\n",
    "                \"id\": class_id,\n",
    "                \"name\": class_name,\n",
    "                \"meta_dataframe\": class_df,\n",
    "            }\n",
    "\n",
    "            if hierarchy_df is not None:\n",
    "                result_dict[\"hierarchy\"] = hierarchy_df\n",
    "            \n",
    "            result_dfs.append(result_dict)\n",
    "        \n",
    "        return result_dfs\n",
    "\n",
    "    def _store_params_in_attrs(self, json_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Store params in attributes as instance variables.\"\"\"\n",
    "        # GET_META_INFOセクションを取得\n",
    "        meta_info = json_data.get(\"GET_META_INFO\", {})\n",
    "        \n",
    "        # RESULTセクションの処理\n",
    "        result = meta_info.get(\"RESULT\", {})\n",
    "        self.STATUS = result.get(\"STATUS\")\n",
    "        self.ERROR_MSG = result.get(\"ERROR_MSG\")\n",
    "        self.DATE = result.get(\"DATE\")\n",
    "\n",
    "        # PARAMETERセクションの処理\n",
    "        parameter = meta_info.get(\"PARAMETER\", {})\n",
    "        self.LANG = parameter.get(\"LANG\")\n",
    "        self.DATA_FORMAT = parameter.get(\"DATA_FORMAT\")\n",
    "\n",
    "        # METADATA_INFセクションの取得\n",
    "        metadata_inf = meta_info.get(\"METADATA_INF\", {})\n",
    "        \n",
    "        # TABLE_INFセクションの処理\n",
    "        table_inf = metadata_inf.get(\"TABLE_INF\", {})\n",
    "        self.TABLE_INF = table_inf\n",
    "        \n",
    "        # Store individual table attributes\n",
    "        table_attributes = [\n",
    "            \"STAT_NAME\", \"GOV_ORG\", \"STATISTICS_NAME\", \"TITLE\", \"CYCLE\",\n",
    "            \"SURVEY_DATE\", \"OPEN_DATE\", \"SMALL_AREA\", \"COLLECT_AREA\",\n",
    "            \"MAIN_CATEGORY\", \"SUB_CATEGORY\", \"OVERALL_TOTAL_NUMBER\",\n",
    "            \"UPDATED_DATE\", \"STATISTICS_NAME_SPEC\", \"TABULATION_SUB_CATEGORY1\",\n",
    "            \"DESCRIPTION\", \"TITLE_SPEC\"\n",
    "        ]\n",
    "        \n",
    "        for attr in table_attributes:\n",
    "            setattr(self, attr, table_inf.get(attr))\n",
    "\n",
    "    def _create_class_dataframe_raw(self, class_data: Union[List[Dict[str, Any]], Dict[str, Any]], class_obj: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create raw DataFrame from class data without column transformations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        class_data : Union[List[Dict[str, Any]], Dict[str, Any]]\n",
    "            Class data from API response (can be list or dict)\n",
    "        class_obj : Dict[str, Any]\n",
    "            Class object metadata\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Optional[pd.DataFrame]\n",
    "            Raw DataFrame created from class data, or None if failed\n",
    "        \"\"\"\n",
    "        if not class_data:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Handle different types of class_data\n",
    "            if isinstance(class_data, list):\n",
    "                df = pd.DataFrame(class_data)\n",
    "            elif isinstance(class_data, dict):\n",
    "                df = pd.DataFrame(pd.Series(class_data)).T\n",
    "            else:\n",
    "                print(f\"CLASS_INF>CLASS_OBJ>CLASSの型: {type(class_data)}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert level to int if exists, handle empty strings\n",
    "            if \"@level\" in df.columns:\n",
    "                # Replace empty strings with NaN, then convert to nullable int\n",
    "                df = df.assign(**{\n",
    "                    \"@level\": lambda d: pd.to_numeric(d[\"@level\"].replace(\"\", pd.NA), errors=\"coerce\").astype(\"Int64\")\n",
    "                })\n",
    "\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating raw DataFrame for class {class_obj.get('@id', 'unknown')}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _apply_column_transformations(self, df: pd.DataFrame, class_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply column name transformations to DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Raw DataFrame\n",
    "        class_name : str\n",
    "            Class name for prefixing\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with transformed column names\n",
    "        \"\"\"\n",
    "        # Create a copy to avoid modifying the original\n",
    "        transformed_df = df.copy()\n",
    "        \n",
    "        # Rename columns with class name prefix\n",
    "        if self.prefix_colname_with_classname:\n",
    "            # クラス名をプレフィックスとして付加\n",
    "            transformed_df = transformed_df.rename(columns=lambda col: f\"{class_name}{col.lstrip('@')}\")\n",
    "        else:\n",
    "            # プレフィックスなし、@記号のみ除去\n",
    "            transformed_df = transformed_df.rename(columns=lambda col: f\"{col.lstrip('@')}\")\n",
    "\n",
    "        if self.lang is None or self.lang != \"E\":\n",
    "            # Convert column names to Japanese\n",
    "            transformed_df = colname_to_japanese(transformed_df).rename(columns={\"\": class_name})\n",
    "\n",
    "        return transformed_df\n",
    "\n",
    "    def _create_class_dataframe(self, class_data: Union[List[Dict[str, Any]], Dict[str, Any]], class_obj: Dict[str, Any]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create DataFrame from class data with full transformations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        class_data : Union[List[Dict[str, Any]], Dict[str, Any]]\n",
    "            Class data from API response (can be list or dict)\n",
    "        class_obj : Dict[str, Any]\n",
    "            Class object metadata\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Optional[pd.DataFrame]\n",
    "            DataFrame created from class data, or None if failed\n",
    "        \"\"\"\n",
    "        # クラス名の取得（@name → @code の順で試行）\n",
    "        class_name = class_obj.get(\"@name\")\n",
    "        if not class_name:\n",
    "            print(f\"警告: クラス名（@name）が見つかりません。処理をスキップします。クラスID: {class_obj.get('@id', 'unknown')}\")\n",
    "            return None\n",
    "        \n",
    "        # Get raw dataframe\n",
    "        raw_df = self._create_class_dataframe_raw(class_data, class_obj)\n",
    "        if raw_df is None:\n",
    "            return None\n",
    "        \n",
    "        # Apply transformations\n",
    "        return self._apply_column_transformations(raw_df, class_name)\n",
    "\n",
    "    def _create_hierarchy_dataframe(self, metainfo: Dict[str, Any], cat_key: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create a hierarchical DataFrame based on metadata information.\n",
    "        \n",
    "        This method creates a DataFrame where each row represents a bottom-level node\n",
    "        in the hierarchy, with columns for each hierarchical level containing \n",
    "        \"code_name\" format values. Missing intermediate levels are forward-filled.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metainfo : Dict[str, Any]\n",
    "            Metadata information containing hierarchical data with @code, @name, \n",
    "            @level, and @parentCode fields\n",
    "        cat_key : int\n",
    "            Target category key index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[pd.DataFrame]\n",
    "            Hierarchical DataFrame with bottom-level nodes as rows and \n",
    "            hierarchical levels as columns, or None if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract target category metadata\n",
    "            class_obj_list = metainfo[\"GET_META_INFO\"][\"METADATA_INF\"][\"CLASS_INF\"][\"CLASS_OBJ\"]\n",
    "            \n",
    "            if cat_key >= len(class_obj_list):\n",
    "                print(f\"警告: cat_key {cat_key} が範囲外です。\")\n",
    "                return None\n",
    "                \n",
    "            cat_meta = class_obj_list[cat_key]\n",
    "            meta_name = cat_meta[\"@name\"]\n",
    "            \n",
    "            class_data = cat_meta.get(\"CLASS\")\n",
    "            if not class_data:\n",
    "                print(f\"警告: CLASS データが見つかりません。\")\n",
    "                return None\n",
    "            \n",
    "            # Handle different types of class_data\n",
    "            if isinstance(class_data, list):\n",
    "                meta_cls_df = pd.DataFrame(class_data)\n",
    "            elif isinstance(class_data, dict):\n",
    "                meta_cls_df = pd.DataFrame(pd.Series(class_data)).T\n",
    "            else:\n",
    "                print(f\"CLASS データの型が不正です: {type(class_data)}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert level to int\n",
    "            if \"@level\" not in meta_cls_df.columns:\n",
    "                print(f\"警告: @level 列が見つかりません。\")\n",
    "                return None\n",
    "                \n",
    "            meta_cls_df = meta_cls_df.assign(\n",
    "                **{\"@level\": lambda df: pd.to_numeric(df[\"@level\"], errors=\"coerce\").astype(\"Int64\")}\n",
    "            )\n",
    "            \n",
    "            # Create set of parent codes for identifying leaf nodes\n",
    "            parent_codes = {\n",
    "                row.get(\"@parentCode\") \n",
    "                for _, row in meta_cls_df.iterrows() \n",
    "                if row.get(\"@parentCode\") and str(row.get(\"@parentCode\")).strip()\n",
    "            }\n",
    "            \n",
    "            # Create code-to-record mapping\n",
    "            code_to_record = {row[\"@code\"]: row for _, row in meta_cls_df.iterrows()}\n",
    "\n",
    "            def _get_ancestry_chain(meta_record: Dict[str, Any]) -> Dict[int, str]:\n",
    "                \"\"\"\n",
    "                Get ancestry chain for a metadata record.\n",
    "                \n",
    "                Parameters\n",
    "                ----------\n",
    "                meta_record : Dict[str, Any]\n",
    "                    Metadata record with @code, @name, @level, @parentCode fields\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                Dict[int, str]\n",
    "                    Dictionary mapping level to code for the ancestry chain\n",
    "                \"\"\"\n",
    "                chain = {}\n",
    "                current_record = meta_record\n",
    "                \n",
    "                while current_record is not None:\n",
    "                    level = current_record[\"@level\"]\n",
    "                    chain[level] = current_record[\"@code\"]\n",
    "                    parent_code = current_record.get(\"@parentCode\")\n",
    "                    \n",
    "                    if not parent_code or parent_code not in code_to_record:\n",
    "                        break\n",
    "                        \n",
    "                    current_record = code_to_record[parent_code]\n",
    "                \n",
    "                return chain\n",
    "\n",
    "            # Process leaf nodes only\n",
    "            max_level = meta_cls_df[\"@level\"].max()\n",
    "            chain_rows = []\n",
    "            \n",
    "            for _, row in meta_cls_df.iterrows():\n",
    "                # Skip parent nodes\n",
    "                if row[\"@code\"] in parent_codes:\n",
    "                    continue\n",
    "\n",
    "                node_level = row[\"@level\"]\n",
    "                ancestry = _get_ancestry_chain(row)\n",
    "                row_chain = {}\n",
    "                last_code = None\n",
    "                \n",
    "                # Build hierarchy with forward fill\n",
    "                for level in range(1, max_level + 1):\n",
    "                    col = f\"level{level}\"\n",
    "                    if level <= node_level:\n",
    "                        if level in ancestry:\n",
    "                            last_code = ancestry[level]\n",
    "                            row_chain[col] = ancestry[level]\n",
    "                        else:\n",
    "                            row_chain[col] = last_code if self.use_fillna_lv_hierarchy else None\n",
    "                    else:\n",
    "                        row_chain[col] = None\n",
    "                        \n",
    "                chain_rows.append(row_chain)\n",
    "\n",
    "            if not chain_rows:\n",
    "                print(f\"警告: 階層データが生成されませんでした。\")\n",
    "                return None\n",
    "\n",
    "            hierarchy_df = pd.DataFrame(chain_rows)\n",
    "\n",
    "            # Merge with names to create \"code_name\" format\n",
    "            for level in range(1, max_level + 1):\n",
    "                level_col = f\"level{level}\"\n",
    "                name_col = f\"{meta_name}階層{level}\"\n",
    "                \n",
    "                name_df = meta_cls_df[[\"@code\", \"@name\"]].assign(\n",
    "                    **{\n",
    "                        level_col: meta_cls_df[\"@code\"],\n",
    "                        name_col: meta_cls_df[\"@code\"] + \"_\" + meta_cls_df[\"@name\"]\n",
    "                    }\n",
    "                )[[\"@code\", name_col]].rename(columns={\"@code\": level_col})\n",
    "                \n",
    "                hierarchy_df = hierarchy_df.merge(name_df, on=level_col, how=\"left\")\n",
    "\n",
    "            # Apply forward fill to both code and name columns if enabled\n",
    "            if self.use_fillna_lv_hierarchy:\n",
    "                level_cols = [f\"level{level}\" for level in range(1, max_level + 1)]\n",
    "                hierarchy_cols = [f\"{meta_name}階層{level}\" for level in range(1, max_level + 1)]\n",
    "                \n",
    "                hierarchy_df[level_cols] = hierarchy_df[level_cols].fillna(method=\"ffill\", axis=1)\n",
    "                hierarchy_df[hierarchy_cols] = hierarchy_df[hierarchy_cols].fillna(method=\"ffill\", axis=1)\n",
    "\n",
    "            return hierarchy_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating hierarchy DataFrame: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf047172-add8-4d8b-a6f7-08cb81b02e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac092d2-4435-4983-8c25-da1fd204ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colname_to_japanese(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert column names to Japanese using non-destructive assign pattern.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with Japanese column names\n",
    "    \"\"\"\n",
    "    def convert_column_name(col: str) -> str:\n",
    "        for k, v in ATTR_DICT.items():\n",
    "            col = col.replace(k, v)\n",
    "        return col\n",
    "    \n",
    "    new_columns = {col: convert_column_name(col) for col in df.columns}\n",
    "    return df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4a5a3-4361-443a-8499-447e08ed5e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc05709-f689-4caa-936b-4f7b1776ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "appId = os.getenv(\"ESTAT_APP_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7ef0b-9ba1-4175-98b8-e0beea89a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDataId = \"0002070010\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c42d6e-8683-47d3-99d0-de3cc0ab2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo = MetaInfoReader(api_key=appId, statsDataId=statsDataId, has_lv_hierarchy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821a54d-63e5-4e21-b5a5-40ec83babaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.url, metainfo.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4e8d7-ed9b-42f0-8bf5-c576db7149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = metainfo._get_response(metainfo.url, params=metainfo.params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba699c-ac35-4c3d-8366-5725666bd598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a76b27-afc9-4937-87cf-32e2f648b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo._store_params_in_attrs(meta_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7b8a3-5eef-4de8-8aca-028907412c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348e3b6-7875-4650-bfed-be4d80634b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj = meta_json[\"GET_META_INFO\"].get(\"METADATA_INF\", {}).get(\"CLASS_INF\", {}).get(\"CLASS_OBJ\", [])\n",
    "class_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b37522-deb0-4dce-8265-62e97ad5c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_obj[1][\"CLASS\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfe68e-da24-4d2c-b39c-94c9336d2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs = metainfo.read_class_obj_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bd54e-bd75-4d3b-a618-5e5bd34567b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544a769-8742-49fd-8c46-854f9890946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs[1]['meta_dataframe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338693e-031c-4458-b0d7-1d9f3df6ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dfs[1]['hierarchy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454512f8-9cb6-410c-bb58-8cfab12df469",
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d2416-7630-40eb-8d49-87cb87a7b64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
